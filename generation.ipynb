{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "i_RsEZ9HVxdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file( \"shakespeare.txt\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\", )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s44a5CWY5gQ",
        "outputId": "8bba7d36-b7cf-4bc1-eb66-726cceceba29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sAKdMbVY5dN",
        "outputId": "0f5cdfae-a8b1-4a92-e11a-181e7fc34c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])   ### The first 250 characters in text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnq7evCYY5ay",
        "outputId": "6abe42c8-e6de-4fd3-8daf-f12239626dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")   ### To see how many unique characters are in our corpus/document."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtLbtGNoY5Ya",
        "outputId": "3474c285-bad1-40cf-b1a3-5dff9bcab67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the text\n"
      ],
      "metadata": {
        "id": "a3kGlV6ZajSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the text"
      ],
      "metadata": {
        "id": "KbGeQpxnanbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "# TODO 1\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPNQCsX_Y5Vy",
        "outputId": "6de6a83e-b43d-49f9-ce23-c6f5f2cc51aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "tdFFo-p7Y5TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtqO6pMYY5Qr",
        "outputId": "1cf4dd33-5060-4697-b2e1-4ff9dc250569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")                 ##This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters:\n",
        "\n"
      ],
      "metadata": {
        "id": "zTk2mdV5bJfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcFD3cWKbQwC",
        "outputId": "b7aeeb22-5e92-44f9-94fe-5054bba43c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()  ###join the characters back to string\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lNBGKSAbQs-",
        "outputId": "e0e4233e-0dfc-412a-8ddf-f5970a26964a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "8blQQ3sRbQqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "z5cewj3HbQn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0598dd-15ab-4f6d-a78b-80693095750d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
      ],
      "metadata": {
        "id": "7GJkTV3E02Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X6HFhgS02Us",
        "outputId": "a802ac44-7fd8-49ea-8ebd-c06dfde33f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "YLvpm7If02SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYnqVrMN02PM",
        "outputId": "cd87b963-a2cb-4bd4-a60c-53338ce5e9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LnI7NSJ02L0",
        "outputId": "ba91bf71-4060-4745-c74e-31298d90e439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "Kb4URMOB27g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhQmYw7d2-eL",
        "outputId": "80239ea0-ee8f-4c29-e898-996896485697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "QERhqS1C2-bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bxkg6wH2-Yk",
        "outputId": "542c90d5-a872-4db0-ba60-c6279085b39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training batches\n"
      ],
      "metadata": {
        "id": "HvS_Ftd73Ohz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCx7HyTj2-WG",
        "outputId": "a974f0df-cbc7-40a1-850a-95c6e03689ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build The Model\n"
      ],
      "metadata": {
        "id": "s-WQfkua3a0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVBJMqnE2-Tr",
        "outputId": "cc19db9a-6e7e-4600-faa6-c97270b06fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        # The following line was not indented properly\n",
        "        x = self.embedding(inputs, training=training)\n",
        "\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        if states is None:\n",
        "            # Initialize the state with the correct shape\n",
        "            states = self.gru.get_initial_state(batch_size=batch_size)\n",
        "\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "K2x9nqOs2-RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "5fYh3otc3wIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IvDzxGg-l4r",
        "outputId": "d26c0f3c-afb2-4a8c-b1f5-b88e11cbfcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_units"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ9V4vaS-qUJ",
        "outputId": "a6e859e5-769a-4019-d6f0-c494de847dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gqsSHNru-l00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvDluoU7-IO7",
        "outputId": "4653db02-fc6e-4f88-9121-939a8fb49f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoFn29434RPu",
        "outputId": "5676498f-6920-4241-bb6b-e4892ab4069c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "fE4lLAf73wFH",
        "outputId": "fdea1863-31a5-4135-8389-aad816061a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"my_model_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_11 (\u001b[38;5;33mGRU\u001b[0m)                         │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m,      │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "│                                      │ \u001b[38;5;34m1024\u001b[0m))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)               │          \u001b[38;5;34m67,650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                         │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "ucEb4TLZ3wC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDWTlVxA3v98",
        "outputId": "ab9727f7-cbbd-48bb-b3fd-c13ef05e66ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 60, 30, 27, 21, 37, 32, 30,  8, 56, 54, 41,  6,  8, 10, 27, 55,\n",
              "       24, 45, 41,  3, 18, 45, 57, 45,  1, 51, 31, 31, 51, 55, 64, 39, 60,\n",
              "       47, 33, 53, 36, 60, 45, 63,  1, 62,  3, 22, 28, 46, 24, 21, 29, 13,\n",
              "        4, 33, 15, 29, 56,  8, 28,  4,  1, 63, 45, 56, 20, 47, 29,  7, 10,\n",
              "       20, 51, 16, 15, 27, 64, 52, 17,  5, 64, 40, 10, 21, 44, 32, 20, 61,\n",
              "       48, 60, 41, 37, 52,  6, 25, 15, 18,  6,  1, 62, 44, 47,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtx_qgao3v7f",
        "outputId": "409fa7d6-4933-4f6a-e4ac-6c92e9acc026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"in place.\\n\\nPost:\\n'Tell him,' quoth she, 'my mourning weeds are done,\\nAnd I am ready to put armour on\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"!uQNHXSQ-qob'-3NpKfb!Efrf\\nlRRlpyZuhTnWufx\\nw!IOgKHP?$TBPq-O$\\nxfqGhP,3GlCBNymD&ya3HeSGviubXm'LBE'\\nweh\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model\n"
      ],
      "metadata": {
        "id": "gtr3lUhWBO_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "id": "uogMZltn2-Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m36nqGLBVZy",
        "outputId": "f9edc858-deb8-4caf-fa83-8706faa7c0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1892962, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CYvSfV-BVWy",
        "outputId": "7d574542-e165-436e-fea0-ef4c35fdfa71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.97634"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)\n"
      ],
      "metadata": {
        "id": "gI014FSIBmIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "# Name of the checkpoint files\n",
        "# Added '.weights.h5' to the checkpoint_prefix\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "BTMwel_lBmFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or4FotwEBmC9",
        "outputId": "57197532-908b-4629-dfe2-995a002ca215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1023s\u001b[0m 6s/step - loss: 3.0596\n",
            "Epoch 2/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1037s\u001b[0m 6s/step - loss: 1.9170\n",
            "Epoch 3/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m999s\u001b[0m 6s/step - loss: 1.6271\n",
            "Epoch 4/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1049s\u001b[0m 6s/step - loss: 1.4795\n",
            "Epoch 5/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1043s\u001b[0m 6s/step - loss: 1.3930\n",
            "Epoch 6/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1030s\u001b[0m 6s/step - loss: 1.3303\n",
            "Epoch 7/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1046s\u001b[0m 6s/step - loss: 1.2796\n",
            "Epoch 8/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1041s\u001b[0m 6s/step - loss: 1.2346\n",
            "Epoch 9/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m992s\u001b[0m 6s/step - loss: 1.1934\n",
            "Epoch 10/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1035s\u001b[0m 6s/step - loss: 1.1489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "uT3Fm9eXBmAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
      ],
      "metadata": {
        "id": "-JCGgwfRFqH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCMIhlR3FqFE",
        "outputId": "7ce8cd46-fcf5-415e-9362-06564bb6a9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Small robe as a good to? Women, you warrant, king, where's my sister.\n",
            "\n",
            "ISABELLA:\n",
            "Whither? Hast yet daynigh, then darest not: ho,\n",
            "Whose difteen need my brother Montague,\n",
            "Since hearing so harth from the measure;\n",
            "He hath closely to my very hath, did them not.\n",
            "Must cure my life, and of your eyes for revenue,\n",
            "I children withar; one courre I have hall:\n",
            "Ay, and you shall pierce in his noble,--\n",
            "ANTONIO:\n",
            "How now I preteous chielded 'gid!\n",
            "\n",
            "TRANIO:\n",
            "What is it? sir? this god? thou hast rark'd you?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I will be cold your prison.\n",
            "\n",
            "ANGELO:\n",
            "Now, hath a man doth say a montal's child, and that\n",
            "I say, a word of newn. Hast thou, Warwick coming! Now, my lord,\n",
            "By stain with thine easy my princely groats,\n",
            "He is now prosperise o' the while: your absequon.\n",
            "\n",
            "LARTIUS:\n",
            "Hark, I created: milter,\n",
            "Saferit, point you.' Nay, a monstrous answer;\n",
            "but though 'tis countenhing ene.'\n",
            "Kith glory, the king's give than a intinent.\n",
            "\n",
            "THOMASES:\n",
            "I begged my deed. And I am not for your comperage.\n",
            "\n",
            "HASTINGS:\n",
            "Spritthell \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 7.965637922286987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUAQHvb7FqCA",
        "outputId": "235efd53-d906-4cb6-c62d-8a2a181f7efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nNay, none! but now to take him woman. Be not, long.\\nTo-minder, I love to see your lace.\\nThe third of my daughter King Richard?\\nYou are flateous to begin and figst\\nMy intending parts and gentlemen,\\nMakes our audicles call take it me bown tigle,\\nHis citizeness to the Duke of Naples\\nWe'll be the bastle of thy mind:\\nAll put their hatres; and, in the words are hither\\nHe's all aught. I six on my head.\\n\\nPAgo: Here comes Messance; I'll be gone.\\n\\nJOHN OF GAUNT:\\nIt is to save them to his close men.\\nWhat news am I take up the controast?\\nO, for what thou be preteceated bore than\\nI have to't brankful blaze the nurted\\nTickle and bitter burdy.\\n\\nServant:\\nMeantily that you good wholes of this, love him up\\nAnd think but while posting to the city behind\\nMy nation deny more shrew that you should be a grace.\\n\\nQUEEN ELIZABETH:\\nWhile we envying them, whose easies are nurse\\ntrue, to think your will till you live to Faint: a sweetest\\nprecious pitchpour lusty figure in my heir,\\nNot taints are but our tife and \"\n",
            " b\"ROMEO:\\nYou are they are not: if you have him eyes' sight.\\n\\nDUCHESS OF YORK:\\nIver well your good own gentle queen's paint.\\nBut whose gables' masquetion and I\\nCut which was bring to your own pieces? If you\\nHere muster night?\\n\\nFRIAR LAURENCE:\\nWhat news with child two piece of this aquate,\\nWe must to her friends at him. This is the constal away,\\nWhereines, should chance again, and obhort'st me.\\n\\nBIONDELLO:\\nNone, since that they pardon these goes are.\\n\\nGhost Citizen:\\nYour willing good Cominius, in the hard.\\nAnd lead-up me.\\nNay, let me hear them an.\\n\\nALONSO:\\nThen say I shall not birrows, every sitt\\nthou remote\\nLues threaten Perch and my time shall be.\\nHere in their eyes, his son be ashamed,\\nAnd yet we owe lordward outhard with a sep mock'd.\\n\\nFLORIZEL:\\nMelenting him with such a coward,\\nThink'ster for you, have answer'd,\\nHe cares like a tonget, and hither!\\n\\nANPEROM:\\nAll question. Lords, what measure is a very\\nI see them well; but I can have 'part'd,\\nOur strange hearts! by gracious moon-crow'd in thi\"\n",
            " b\"ROMEO:\\nWhose natures shut up in earth? alas, at them?\\n\\nCALIBAN:\\nYe' so I say; I, in the cause of yours\\nShall be proudivy to your people.\\n\\nHASTINGS:\\nWhose year sons called may will tell it hither?\\nho, thou'-to suck no rogue maid?\\nHe cannot prove the duty! a rampliar office\\nTo thousand to him through the light: all which, a-right,\\nWas a desert and milful sincertain:\\nWere he on thy kingdoms and a Richard's cheeks\\nWhich now hath soothed against that thine!\\n\\nHENRY BOLINGBROKE:\\nIt whiles we beg.\\n\\nDORSET:\\nTut, she'll like your womb, here, by the fateless groans.\\n\\nGONZALO:\\nThus is harthless gentleman. There is me not:\\nHow naws, this is the rights either, be the due!\\n\\nCAPULET:\\nNight, cait, no furility down to-night.\\nMy business upon the messeaking subs\\nOf time together, ay, and in despite. To the time in wares\\nDivines his happiness: being be, to our duke\\nshed comansa e'er I with toing.\\n\\nANTONIO:\\nMy mean of life; might you gurst.\\n\\nLADY ANNE:\\nNo, indeed, I beseech you; pray you, sir, to your threatsher\"\n",
            " b\"ROMEO:\\nIs the cools have proble into\\nWithin the hatter an eye of.\\n\\nESCALUS:\\nGood Rome, Paulina, Warwick: and you at,\\nCommanding them themselves. He, have too honour!\\n\\nAll:\\nAnd waste! How trequards aloud go;\\nTherefore King Richard stands: I am book coals\\nHad we have known'd or unranning, you mighty lies\\nOne Italy and four presention.\\nHe cases it not, part off! my Lord SCaleas:\\nNe respect.\\n\\nCORIOLANUS:\\nSir, fie! the banish'd Hastings, lords\\nWith brights all mine. Stand all our sinel sure,\\nLike against Our bowling. Can thou threating, why, am not\\nOr your own careless beauty's life,\\nHis royalts and unto this deek-repering.\\nWith this afternoon, gave me not!\\n\\nJOHN OF GAUNT:\\nI do promise, make it not. I never did but in\\nThe corn I never cried with Bolingbroke,\\nMasquing to a weapon. Welcome! all to the earth\\nIt was the tidew of her chmerching horrible\\nIs manner had nothing but mightshy, let me point.\\n\\nHENRY BOLINGBROKE:\\nHere, be sured.\\n\\nMARIANA:\\nThat Rome, come asking turn; but I shall\\nMore than tho\"\n",
            " b\"ROMEO:\\nHis chings and thy canclive was judgled star,\\nSaw many throught to all come again and here be hangedon.\\n\\nISABELLA:\\n\\nPOMPEY:\\nHow fares you give me learn'd, and told, let us grief\\nThat makes mo strikes o' the city against you.\\nHe didst they are to me, and be merr'd.\\n\\nANTERONS:\\nIndeed, I'll make him all.\\n\\nQUEEN MARGARET:\\nAway, bade me and Isabed and to Montague,\\nThat Richard stand you meet have strain'd,\\nUntil the counsel? To time why I were discharget.\\nMy nights for reverence on the creatue!\\n\\nWARWICK:\\nNow, triumph, my lord, sir, noce\\nthat with sweet cords with flower.\\n\\nSLY:\\nMarry, Isabel; come, you have answer to follow:\\nthough notoble Northumbelless; the window-sheather,\\nI do talk of run, how many hatch abused\\nwith our honour, thou, and I will speak but thy day.\\nRichard from tune with your graunty!'s faith, my master, yet,\\nI fongot my prison reKersure for me and content,\\nYourshing is notict and weal; and all enjountaman\\nClaudior, to the town and that I will entire:\\nIt's a gentleman of \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.041520118713379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the generator\n"
      ],
      "metadata": {
        "id": "j5HmNFl_F86k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ],
      "metadata": {
        "id": "Xc_8cPuMFp_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqGrQ17KFp8g",
        "outputId": "d5cc476e-feb2-4c33-b384-badce05af571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Thou say, none as first loed of! Gentle king,\n",
            "Where pretty time without mine: you are full of twate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced: Customized Training\n"
      ],
      "metadata": {
        "id": "s_rumoH4GOEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "0ZE4ZK5iFp6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "Ugkx3iDZFp3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")"
      ],
      "metadata": {
        "id": "a2kvuAhvFp0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CYuG1k4Fpxf",
        "outputId": "dd1aaba4-794f-4c18-87a3-cf485388e807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1006s\u001b[0m 6s/step - loss: 2.4928\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4c50542d10>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # Use reset_state() instead of reset_states()\n",
        "    mean.reset_state()\n",
        "    for batch_n, (inp, target) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs[\"loss\"])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = (\n",
        "                f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            )\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f\"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}\")\n",
        "    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n",
        "    print(\"_\" * 80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83sbWCXZGjvm",
        "outputId": "235451cf-56f4-465e-d7ae-67b89ccf6d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0487\n",
            "Epoch 1 Batch 50 Loss 1.9394\n",
            "Epoch 1 Batch 100 Loss 1.7911\n",
            "Epoch 1 Batch 150 Loss 1.7294\n",
            "\n",
            "Epoch 1 Loss: 1.8335\n",
            "Time taken for 1 epoch 1041.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7042\n",
            "Epoch 2 Batch 50 Loss 1.6110\n",
            "Epoch 2 Batch 100 Loss 1.5343\n",
            "Epoch 2 Batch 150 Loss 1.5604\n",
            "\n",
            "Epoch 2 Loss: 1.5929\n",
            "Time taken for 1 epoch 1041.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.4831\n",
            "Epoch 3 Batch 50 Loss 1.4531\n",
            "Epoch 3 Batch 100 Loss 1.5023\n",
            "Epoch 3 Batch 150 Loss 1.4387\n",
            "\n",
            "Epoch 3 Loss: 1.4667\n",
            "Time taken for 1 epoch 998.72 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.3768\n",
            "Epoch 4 Batch 50 Loss 1.4006\n",
            "Epoch 4 Batch 100 Loss 1.3997\n",
            "Epoch 4 Batch 150 Loss 1.3731\n",
            "\n",
            "Epoch 4 Loss: 1.3887\n",
            "Time taken for 1 epoch 1041.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3201\n",
            "Epoch 5 Batch 50 Loss 1.3437\n",
            "Epoch 5 Batch 100 Loss 1.3758\n",
            "Epoch 5 Batch 150 Loss 1.3178\n",
            "\n",
            "Epoch 5 Loss: 1.3324\n",
            "Time taken for 1 epoch 1001.85 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2628\n",
            "Epoch 6 Batch 50 Loss 1.3123\n",
            "Epoch 6 Batch 100 Loss 1.3372\n",
            "Epoch 6 Batch 150 Loss 1.2613\n",
            "\n",
            "Epoch 6 Loss: 1.2865\n",
            "Time taken for 1 epoch 1007.79 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2347\n",
            "Epoch 7 Batch 50 Loss 1.2306\n",
            "Epoch 7 Batch 100 Loss 1.2037\n",
            "Epoch 7 Batch 150 Loss 1.2653\n",
            "\n",
            "Epoch 7 Loss: 1.2450\n",
            "Time taken for 1 epoch 1041.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.1990\n",
            "Epoch 8 Batch 50 Loss 1.2306\n",
            "Epoch 8 Batch 100 Loss 1.2493\n",
            "Epoch 8 Batch 150 Loss 1.2217\n",
            "\n",
            "Epoch 8 Loss: 1.2060\n",
            "Time taken for 1 epoch 994.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1521\n",
            "Epoch 9 Batch 50 Loss 1.1851\n",
            "Epoch 9 Batch 100 Loss 1.1658\n",
            "Epoch 9 Batch 150 Loss 1.1982\n",
            "\n",
            "Epoch 9 Loss: 1.1674\n",
            "Time taken for 1 epoch 1005.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.0840\n",
            "Epoch 10 Batch 50 Loss 1.1108\n",
            "Epoch 10 Batch 100 Loss 1.1474\n",
            "Epoch 10 Batch 150 Loss 1.1299\n",
            "\n",
            "Epoch 10 Loss: 1.1271\n",
            "Time taken for 1 epoch 1042.11 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}